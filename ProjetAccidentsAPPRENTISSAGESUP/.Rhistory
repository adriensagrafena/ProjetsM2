mois=month(date,label=TRUE,abbr=FALSE,locale = Sys.getlocale("LC_TIME")),
annee=year(date)
)->data
mycolor <- c("#2780e3","#003636","#a9a9a9", "#91c8c8")
View(data_text)
library("rmarkdown")
library("flexdashboard")
library("ggplot2")
library("ggplot2")
library("plotly")
library("ggplot2")
library("plotly")
library("knitr")
library("xml2")
library("readr")
library("lubridate")
library("tidyverse")
library("tidyverse")
library("stringr")
library("tidytext")
library("tidyverse")
library("stringr")
library("tidytext")
library("shiny")
setwd("~/projet-rss")
data_text <- read_delim("data/data_text.csv",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
prev<- Sys.getlocale("LC_TIME") # pour éviter des ennuis de conversion pénibles
Sys.setlocale("LC_TIME", "C")   # on passe de "French_France.1252" à "C"
data_text$date=strptime(data_text$date,format ="%B %d, %Y")
Sys.setlocale("LC_TIME", prev)  # retour système local
data_text%>%
mutate(
jour=wday(date,label = TRUE,abbr = FALSE ,week_start = "1",locale = Sys.getlocale("LC_TIME")),
mois=month(date,label=TRUE,abbr=FALSE,locale = Sys.getlocale("LC_TIME")),
annee=year(date)
)->data_text
```{r}
ui=fluidPage(
tabsetPanel(
tabPanel("Par an", fluid = TRUE,
sidebarLayout(
sidebarPanel(textInput("mot",label="Choix du mot/de l'expression",
value = "data",width=NULL,
placeholder = "merci d'entrer un mot ou un groupe de mots ")),
mainPanel(echarts4rOutput("anneeplot"))
)
),
tabPanel("par mois", fluid = TRUE,
sidebarLayout(
sidebarPanel(textInput("mot",label="Choix du mot/de l'expression",
value = "data",width=NULL,
placeholder = "merci d'entrer un mot ou un groupe de mots "),
sliderInput("annee", label = "Choix de l'année",
min = 2008, max = 2021,value=2015 ,step = 1),width=3),
mainPanel(echarts4rOutput("moisplot"))
)
),
tabPanel("par auteur", fluid = TRUE,
sidebarLayout(
sidebarPanel(textInput("mot",label="Choix du mot/de l'expression",
value = "data",width=NULL,
placeholder = "merci d'entrer un mot ou un groupe de mots "),
selectizeInput("author", label = "Choix de l'auteur",
choices = unique(sort(data_text$author)),
selected = "Antoine Guillot")
),
mainPanel(echarts4rOutput("authorplot"))
)
)
)
)
shinyApp(ui = ui, server = server)
library("echarts4r")
install.packages(c("profvis", "roxygen2", "usethis", "Rcpp"))
install.packages(c("jpeg", "microbenchmark", "usethis"))
library(xlsx)
LAozoneData <- read.xlsx(file.choose(), sheetIndex = 1, header = T)
library(ggplot2)
library(glmnet)
library(diplyr)
set.seed(123)
n=nrow(LAozoneData)
index = sample(1:n, 0.7*n)
str(LAozoneData)
head(LAozoneData)
library(ggplot2)
library(glmnet)
set.seed(123)
n=nrow(LAozoneData)
index = sample(1:n, 0.7*n)
train =LAozoneData[index,] # Create the training data
test = LAozoneData[-index,]
LAozoneData.mat <- as.matrix(train)
reg.ridge <- glmnet(x = scale(LAozoneData.mat[,2:10]), y = LAozoneData.mat[,1], alpha = 0)
par(mfrow = c(1,2))
plot(reg.ridge, label = TRUE)
plot(reg.ridge, xvar = "lambda", label = TRUE, lwd = 2)
reg.cvridge <- cv.glmnet(x = scale(LAozoneData.mat[,2:10]), y = LAozoneData.mat[,1],
alpha = 0)
bestlam <- reg.cvridge$lambda.min
par(mfrow = c(1,2))
plot(reg.cvridge)
min(reg.cvridge$cvm) #erreur de prevision du modele ridge optimal sur base train
coef(reg.cvridge)
LAozoneData.test.mat<-as.matrix(test)
reg.cvridge2 <- cv.glmnet(x = scale(LAozoneData.test.mat[,2:10]), y = LAozoneData.test.mat[,1],
alpha = 0)
min(reg.cvridge2$cvm)    #erreur de prevision du modele ridge optimal sur base test
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
#Prediction and evaluation on train data
predictions_train <- predict(reg.ridge, s = bestlam, newx =LAozoneData.mat[,2:10] )
eval_results(train[,1], predictions_train, train)
# Prediction and evaluation on test data
predictions_test <- predict(reg.ridge, s = bestlam, newx =LAozoneData.test.mat[,2:10] )
eval_results(test[,1], predictions_test, test)
str(LAozoneData)
head(LAozoneData)
library(ggplot2)
library(glmnet)
set.seed(123)
n=nrow(LAozoneData)
index = sample(1:n, 0.7*n)
train =LAozoneData[index,] # Create the training data
test = LAozoneData[-index,]
LAozoneData.mat <- as.matrix(train)
reg.ridge <- glmnet(x = scale(LAozoneData.mat[,2:10]), y = LAozoneData.mat[,1], alpha = 0)
par(mfrow = c(1,2))
plot(reg.ridge, label = TRUE)
plot(reg.ridge, xvar = "lambda", label = TRUE, lwd = 2)
reg.cvridge <- cv.glmnet(x = scale(LAozoneData.mat[,2:10]), y = LAozoneData.mat[,1],
alpha = 0)
bestlam <- reg.cvridge$lambda.min
par(mfrow = c(1,2))
plot(reg.cvridge)
min(reg.cvridge$cvm) #erreur de prevision du modele ridge optimal sur base train
coef(reg.cvridge)
LAozoneData.test.mat<-as.matrix(test)
reg.cvridge2 <- cv.glmnet(x = scale(LAozoneData.test.mat[,2:10]), y = LAozoneData.test.mat[,1],
alpha = 0)
min(reg.cvridge2$cvm)    #erreur de prevision du modele ridge optimal sur base test
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
#Prediction and evaluation on train data
predictions_train <- predict(reg.ridge, s = bestlam, newx =LAozoneData.mat[,2:10] )
eval_results(train[,1], predictions_train, train)
# Prediction and evaluation on test data
predictions_test <- predict(reg.ridge, s = bestlam, newx =LAozoneData.test.mat[,2:10] )
eval_results(test[,1], predictions_test, test)
index = sample(1:n, 0.7*n)
train =LAozoneData[index,] # Create the training data
test = LAozoneData[-index,]
LAozoneData.mat <- as.matrix(train)
reg.ridge <- glmnet(x = scale(LAozoneData.mat[,2:10]), y = LAozoneData.mat[,1], alpha = 0)
par(mfrow = c(1,2))
plot(reg.ridge, label = TRUE)
plot(reg.ridge, xvar = "lambda", label = TRUE, lwd = 2)
reg.cvridge <- cv.glmnet(x = scale(LAozoneData.mat[,2:10]), y = LAozoneData.mat[,1],
alpha = 0)
bestlam <- reg.cvridge$lambda.min
par(mfrow = c(1,2))
plot(reg.cvridge)
min(reg.cvridge$cvm) #erreur de prevision du modele ridge optimal sur base train
coef(reg.cvridge)
LAozoneData.test.mat<-as.matrix(test)
reg.cvridge2 <- cv.glmnet(x = scale(LAozoneData.test.mat[,2:10]), y = LAozoneData.test.mat[,1],
alpha = 0)
min(reg.cvridge2$cvm)    #erreur de prevision du modele ridge optimal sur base test
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
#Prediction and evaluation on train data
predictions_train <- predict(reg.ridge, s = bestlam, newx =LAozoneData.mat[,2:10] )
eval_results(train[,1], predictions_train, train)
# Prediction and evaluation on test data
predictions_test <- predict(reg.ridge, s = bestlam, newx =LAozoneData.test.mat[,2:10] )
eval_results(test[,1], predictions_test, test)
LAozoneData <- read.xlsx(file.choose(), sheetIndex = 1, header = T)
str(LAozoneData)
head(LAozoneData)
library(ggplot2)
library(glmnet)
set.seed(123)
n=nrow(LAozoneData)
index = sample(1:n, 0.7*n)
train =LAozoneData[index,] # Create the training data
test = LAozoneData[-index,]
LAozoneData.mat <- as.matrix(train)
reg.ridge <- glmnet(x = scale(LAozoneData.mat[,2:10]), y = LAozoneData.mat[,1], alpha = 0)
par(mfrow = c(1,2))
plot(reg.ridge, label = TRUE)
plot(reg.ridge, xvar = "lambda", label = TRUE, lwd = 2)
reg.cvridge <- cv.glmnet(x = scale(LAozoneData.mat[,2:10]), y = LAozoneData.mat[,1],
alpha = 0)
bestlam <- reg.cvridge$lambda.min
par(mfrow = c(1,2))
plot(reg.cvridge)
min(reg.cvridge$cvm) #erreur de prevision du modele ridge optimal sur base train
coef(reg.cvridge)
LAozoneData.test.mat<-as.matrix(test)
reg.cvridge2 <- cv.glmnet(x = scale(LAozoneData.test.mat[,2:10]), y = LAozoneData.test.mat[,1],
alpha = 0)
min(reg.cvridge2$cvm)    #erreur de prevision du modele ridge optimal sur base test
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics
data.frame(
RMSE = RMSE,
Rsquare = R_square
)
}
#Prediction and evaluation on train data
predictions_train <- predict(reg.ridge, s = bestlam, newx =LAozoneData.mat[,2:10] )
eval_results(train[,1], predictions_train, train)
# Prediction and evaluation on test data
predictions_test <- predict(reg.ridge, s = bestlam, newx =LAozoneData.test.mat[,2:10] )
eval_results(test[,1], predictions_test, test)
cor(LAozoneData)
data<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
library(xlsx)
library(readxl)
# loréal
library('readr')
library('readxl')
library('tidyverse')
library('ggplot2')
data<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
summary(data)
describe(data)
data$panier_moyen <- data$sell_out/data$nb_transactions
data$tx_open <- data$nb_open / data$nb_targets
data$tx_click <- data$nb_click / data$nb_targets
data$tx_desabo <- data$nb_desabo / data$nb_targets
View(data)
dtf<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
dtf<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
summary(data)
describe(data)
dtf<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
dtf$panier_moyen <- dtf$sell_out/dtf$nb_transactions
dtf$tx_open <- dtf$nb_open / dtf$nb_targets
dtf$tx_click <- dtf$nb_click / dtf$nb_targets
dtf$tx_desabo <- dtf$nb_desabo / dtf$nb_targets
set.seed(123)
n=nrow(dtf)
index = sample(1:n, 0.7*n)
train =LAozoneData[index,] # Create the training data
# loréal
library('readr')
library('readxl')
library('tidyverse')
library('ggplot2')
dtf<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
summary(data)
describe(data)
dtf$panier_moyen <- dtf$sell_out/dtf$nb_transactions
dtf$tx_open <- dtf$nb_open / dtf$nb_targets
dtf$tx_click <- dtf$nb_click / dtf$nb_targets
dtf$tx_desabo <- dtf$nb_desabo / dtf$nb_targets
set.seed(123)
n=nrow(dtf)
index = sample(1:n, 0.7*n)
train =dtf[index,] # Create the training data
test = dtf[-index,]
dtf
View(dtf)
dtf<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
summary(data)
describe(data)
dtf$panier_moyen <- dtf$sell_out/dtf$nb_transactions
dtf$tx_open <- dtf$nb_open / dtf$nb_targets
dtf$tx_click <- dtf$nb_click / dtf$nb_targets
dtf$tx_desabo <- dtf$nb_desabo / dtf$nb_targets
dtf
set.seed(123)
n=nrow(dtf)
index = sample(1:n, 0.7*n)
train =dtf[index,] # Create the training data
test = dtf[-index,]
dtf<-dtf[c(4,8,12,13,14),]
dtf<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
summary(data)
describe(data)
#dtf$panier_moyen <- dtf$sell_out/dtf$nb_transactions
dtf$tx_open <- dtf$nb_open / dtf$nb_targets
dtf$tx_click <- dtf$nb_click / dtf$nb_targets
dtf$tx_desabo <- dtf$nb_desabo / dtf$nb_targets
dtf<-dtf[c(4,8,12,13,14),]
dtf<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
summary(data)
describe(data)
#dtf$panier_moyen <- dtf$sell_out/dtf$nb_transactions
dtf$tx_open <- dtf$nb_open / dtf$nb_targets
dtf$tx_click <- dtf$nb_click / dtf$nb_targets
dtf$tx_desabo <- dtf$nb_desabo / dtf$nb_targets
dtf<-dtf[,c(4,8,12,13,14)]
dtf<-dtf[,c(4,8,12,13,14)]
dtf<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
#dtf$panier_moyen <- dtf$sell_out/dtf$nb_transactions
dtf$tx_open <- dtf$nb_open / dtf$nb_targets
dtf$tx_click <- dtf$nb_click / dtf$nb_targets
dtf$tx_desabo <- dtf$nb_desabo / dtf$nb_targets
dtf<-dtf[,c(4,8,11,12,13)]
dtf<-dtf[,c(4,8,11,12,13)]
set.seed(123)
n=nrow(dtf)
index = sample(1:n, 0.7*n)
train =dtf[index,] # Create the training data
test = dtf[-index,]
dtf.mat <- as.matrix(train)
set.seed(123)
n=nrow(dtf)
index = sample(1:n, 0.7*n)
train =dtf[index,] # Create the training data
test = dtf[-index,]
dtf.mat <- as.matrix(train)
reg.ridge <- glmnet(x = scale(dtf.mat[,c(1,3:5)]), y = dtf.mat[,2], alpha = 0)
par(mfrow = c(1,2))
plot(reg.ridge, label = TRUE)
plot(reg.ridge, xvar = "lambda", label = TRUE, lwd = 2)
reg.cvridge <- cv.glmnet(x = scale(dtf.mat[,c(1,3:5)]), y = dtf.mat[,2],
alpha = 0)
par(mfrow = c(1,2))
plot(reg.ridge, label = TRUE)
plot(reg.ridge, xvar = "lambda", label = TRUE, lwd = 2)
reg.cvridge <- cv.glmnet(x = scale(dtf.mat[,c(1,3:5)]), y = dtf.mat[,2],
alpha = 0)
bestlam <- reg.cvridge$lambda.min
par(mfrow = c(1,2))
plot(reg.cvridge)
min(reg.cvridge$cvm) #erreur de prevision du modele ridge optimal sur base train
coef(reg.cvridge)
set.seed(123)
n=nrow(dtf)
n=nrow(dtf)
train =dtf[index,] # Create the training data
test = dtf[-index,]
dtf.mat <- as.matrix(train)
reg.ridge <- glmnet(x = scale(dtf.mat[,c(1,3:5)]), y = dtf.mat[,2], alpha = 0)
par(mfrow = c(1,2))
plot(reg.ridge, label = TRUE)
plot(reg.ridge, xvar = "lambda", label = TRUE, lwd = 2)
par(mfrow = c(1,2))
plot(reg.ridge, label = TRUE)
plot(reg.ridge, xvar = "lambda", label = TRUE, lwd = 2)
reg.cvridge <- cv.glmnet(x = scale(dtf.mat[,c(1,3:5)]), y = dtf.mat[,2],
alpha = 0)
bestlam <- reg.cvridge$lambda.min
min(reg.cvridge$cvm) #erreur de prevision du modele ridge optimal sur base train
coef(reg.cvridge)
cor(dtf)
View(data)
dtf<- read_csv("C:/Users/Adrien/Downloads/extract_audit_lld-1.csv")
#dtf$panier_moyen <- dtf$sell_out/dtf$nb_transactions
dtf$tx_open <- dtf$nb_open / dtf$nb_targets
dtf$tx_click <- dtf$nb_click / dtf$nb_targets
dtf$tx_desabo <- dtf$nb_desabo / dtf$nb_targets
dtf$sell_out<-dtf$sell_out/dtf$nb_transactions
dtf<-dtf[,c(4,8,11,12,13)]
drop_na(data=dtf)
set.seed(123)
n=nrow(dtf)
index = sample(1:n, 0.7*n)
train =dtf[index,] # Create the training data
test = dtf[-index,]
dtf.mat <- as.matrix(train)
reg.ridge <- glmnet(x = scale(dtf.mat[,c(1,3:5)]), y = dtf.mat[,2], alpha = 0)
View(dtf)
dtf<-drop_na(data=dtf)
set.seed(123)
n=nrow(dtf)
index = sample(1:n, 0.7*n)
train =dtf[index,] # Create the training data
test = dtf[-index,]
dtf.mat <- as.matrix(train)
reg.ridge <- glmnet(x = scale(dtf.mat[,c(1,3:5)]), y = dtf.mat[,2], alpha = 0)
par(mfrow = c(1,2))
plot(reg.ridge, label = TRUE)
plot(reg.ridge, xvar = "lambda", label = TRUE, lwd = 2)
reg.cvridge <- cv.glmnet(x = scale(dtf.mat[,c(1,3:5)]), y = dtf.mat[,2],
alpha = 0)
bestlam <- reg.cvridge$lambda.min
par(mfrow = c(1,2))
plot(reg.cvridge)
par(mfrow = c(1,1))
plot(reg.cvridge)
min(reg.cvridge$cvm) #erreur de prevision du modele ridge optimal sur base train
coef(reg.cvridge)
gc()
install.packages("flextable")
install.packages("officer")
install.packages("modeltime")
install.packages("timetk")
install.packages("prophet")
install.packages("gtsummary")
# Premier essai : ajustement d'un modele de classification
library("rpart")
n1<-20
n2<-30
n3<-15
n4<-5
set.seed(123) # important : fixer la graine du générateur
echantillon<-data.frame(rbind(matrix(rnorm(n1*2),ncol=2),
matrix(rnorm(n2*2,mean = 4),ncol=2),
matrix(c(rnorm(n3,mean = 4),rnorm(n3,mean=-4.5)),ncol=2),
matrix(c(rnorm(n4,mean = -2),rnorm(n4,mean=-2.5)),ncol=2)))
names(echantillon)<-c("Variable1","Variable2")
random_clust<-function(df,k){
# on récupère le nombre de lignes du data frame passé en argument de la fonction
n<-nrow(df)
# tirage aléatoire des labels pour chaque individu
r<-sample(seq(1,k), n, replace=TRUE)
# On retourne ce résultat
return(r)
}
labels<-random_clust(echantillon, 4)
print(labels)
plot(echantillon,pch=21,bg=labels)
plot(echantillon,pch=21,bg=c("red", "green", "blue", "magenta")[labels])
quad_clust<-function(df){
# On détermine la médiane de la première variable
med1<-median(df[,1])
# On détermine la médiane de la seconde variable
med2<-median(df[,2])
# Affectons les labels aux données
labels<-rep(0,nrow(df))
for (i in seq(nrow(df))){
if (df[i,1]<med1 ){
if (df[i,2]<med2){
labels[i]<-1
}
else{
labels[i]<-4
}
}else{
if (df[i,2]<med2){
labels[i]<-3
}
else{
labels[i]<-2
}
}
}
return(labels)
}
labels<-quad_clust(echantillon)
print(labels)
plot(echantillon,pch=21,bg=labels)
km<-kmeans(echantillon,centers=3)
plot(echantillon,pch=21,bg=km$cluster)
attributes(km)
hca<-hclust(dist(echantillon),method="ward.D")
km<-kmeans(echantillon,centers=3)
plot(echantillon,pch=21,bg=km$cluster)
plot(echantillon,pch=21,bg=labels)
km<-kmeans(echantillon,centers=3)
plot(echantillon,pch=21,bg=km$cluster)
attributes(km)
hca<-hclust(dist(echantillon),method="ward.D")
plot(hca)  # tenter heng=un nombre négatif pr affichage coller axe abscisse
lab<-cutree(hca,3) # pour chaque donnée, on a l'entier qui caractérise le cluster
plot(echantillon,pch=21,bg=lab) #scatterplot en utilisant ce résultat de clustering comme couleur
setwd("~/AccidentsRoutiers")
load("~/ProjetsM2/ProjetAccidentsAPPRENTISSAGESUP/.RData")
plot(res.acm)
plot(res.acm$ind)
# regression logistique multinomiale (car nbr modalités>2 pour la variable cible "grav")
library("nnet")
library("VGAM")
library(FactoMineR)
library(factoextra)
# bibliotheques
library('dplyr')
library('stringr')
library('kableExtra')
setwd("~/ProjetsM2/ProjetAccidentsAPPRENTISSAGESUP")
res.acm<-MCA(df)
library(FactoMineR)
library(factoextra)
View(df)
res.acm<-MCA(df)
